% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tree_predictions.R
\name{tree_predictions}
\alias{tree_predictions}
\title{Get tree predictions for all predictor combinations in random forest}
\usage{
tree_predictions(
  m,
  data,
  num.trees = 500L,
  n.breaks = 10L,
  verbose = TRUE,
  breaks = NULL
)
}
\arguments{
\item{m}{Random forest model. Must be of class \code{ranger} or \code{RandomForest}.}

\item{data}{Dataset used to fit model}

\item{num.trees}{Number of trees from which to extract predictions. Default is 500.}

\item{n.breaks}{Number of breaks with which to split continuous predictors. Default is 10.}

\item{verbose}{Logical. Should information be printed?}

\item{breaks}{A named list with values representing custom points for which to get predictions for continuous predictors. If a number of length = 1 is used, that will be the number of evenly-spaced points used (similar to \code{n.breaks})}

\item{vars}{Character string of targeted predictor variable names from which to derive marginal predictions. Default is all predictors in \code{m} (not recommended).}

\item{ext_vars}{Character string indicating the external variables to take into consideration when weighting}

\item{variable.vals}{Named list containing the values for all variables used to create the reference grid.}
}
\value{
A \code{list} of class \code{treePredictions}.
\describe{
\item{\code{predictions}}{A \code{data.table} of the marginal predictions from the model}
\item{\code{model}}{The name of the model}
\item{\code{data}}{The dataset the model was trained on}
\item{\code{variable_names}}{The names of the independent variables in the model}
\item{\code{predicted.outcome}}{The value representing the positive predicted outcome}
\item{\code{n.breaks}}{The number of breakpoints used for binning continuous predictors}
\item{\code{num.trees}}{The number of trees included in \code{predictions}}
}
}
\description{
Get tree predictions for all predictor combinations in random forest
}
\details{
Add details here
}
\examples{
\dontrun{
# not run
library(ranger)
library(dplyr)

## predict binary outcome

df <- written_genitives |>
dplyr::mutate(
  Type = as.factor(Type),
  Genre = as.factor(Genre),
  Possessor_Animacy3 = as.factor(Possessor_Animacy3),
  Possessor_NP_Type = as.factor(Possessor_NP_Type)
)

rf1 <- ranger(
  Type ~ Genre + Possessor_Animacy3 + Possessor_NP_Type + Possessor_Length,
  data = df,
  mtry = 3,
  num.trees = 500L,
  probability = TRUE
)

tree_preds1 <- tree_predictions(rf1, df)

avg_predictions(tree_preds1, "Possessor_Animacy3")

avg_contrasts(tree_preds1, "Possessor_Animacy3", by = "Genre")


## predict continuous outcome

df2 <- mtcars |>
  mutate(
  cyl = as.factor(cyl),
  gear = as.factor(gear)
  )

rf2 <- ranger(
  mpg ~ .,
  data = mtcars,
  mtry = 3,
  num.trees = 500L,
  probability = TRUE
)

tree_preds2 <- tree_predictions(rf2, mtcars)

avg_predictions(tree_preds1, "wt")

avg_contrasts(tree_preds1, "cyl")

}
}
\references{
SÃ¶nning, Lukas & Jason Grafmiller. 2022. Seeing the wood for the trees: Predictive margins for random forests. Preprint. \emph{PsyArXiv}. \url{https://doi.org/10.31234/osf.io/jr8yk}.
}
\author{
Jason Grafmiller
}
