---
title: "Using predictive margins"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette 
vignette: >
  %\VignetteIndexEntry{Using predictive margins}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 4,
  fig.width = 5
)
```

# Introduction

This vignette demonstrates how to use the functions in the `{predictiveMargins}` package. The aim of the package is to provide interpretable summaries of individual predictor effects in random forest models. There are four main functions in this package.

- `tree_predictions()`: Gets the predictions for all trees in the forest for all possible combinations of predictor values.
- `avg_predictions()`: Calculates the weighted mean prediction for values of specified target predictor variables (similar in spirit to marginal effects or partial dependence plots).
- `predicted_contrasts()`: Calculates the difference (contrast) in predicted outcomes between levels of a target predictor variable.
- `avg_contrasts()`: Calculates the weighted mean of differences in a target variable across levels of one or more secondary variables. 

The basic procedure goes as follows: 

1. Fit a random forest (RF) model. *Note: Currently the package only works with the `{ranger}` and `{party}` packages*
2. Create a new dataframe containing all possible combinations of predictor values in the random forest model. E.g. for three predictors *A*, *B*, and *C*, with $n_A$, $n_B$, $n_C$ levels, we get $n_A \times n_B \times n_C$ combinations.
3. For each combination, calculate the predicted outcome for each tree in the random forest.
4. Summarize the effects of one or more target predictors by averaging over all other predictors, where the averages are weighted by the distributions of the specific predictor combinations in the original dataset.

Steps 2 and 3 are executed in `tree_predictions()`. The other three functions provide different methods for summarizing patterns of specific target predictors (Step 4).

For more details of the method and theoretical motivation see Sönning & Grafmiller (2022), [Seeing the wood for the trees: Predictive margins for random forests](https://doi.org/10.31234/osf.io/jr8yk).

## Libraries

For this vignette we'll use the following basic libraries. 

```{r setup}
library(tidyverse) # for data wrangling and plotting
library(data.table) # used by {predictiveMargins} for much faster data wrangling and computation
```

Install the `{predictiveMargins}` package.

```{r eval = FALSE}
remotes::install_github("jasongraf1/predictiveMargins")
```

Load the library as usual.

```{r}
library(predictiveMargins)
```

We start with random forests for classification cases first, then look at an example with a continuous outcome.  
  

# Predicting discrete outcomes (classification)

In the case of categorical outcomes, our predictions will be the probability of one or more classes.

We'll use actual datasets from linguistic research in this tutorial. We start with the `written_genitives` dataset of English genitive constructions (*the children's voices* [*s*-genitive] vs. *the voices of the children* [*of*-genitive]) included in the package and discussed in Sönning & Grafmiller (2022). See `?written_genitives` for more on the dataset.

We're interested in exploring the features that correlate with use of different genitive constructions. We know, for instance, that animate (e.g. human) possessors tend to co-occur more often with the *s*-genitive, and inanimate possessors more with the *of*-genitive.

```{r}
ggplot(written_genitives, aes(Possessor_Animacy3, fill = Type)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion of observed genitive constructions",
       title = "Genitives by possessor animacy",
       subtitle = "In 1960s and 1990s written American English") +
  theme_classic()
```

So the aim is to build a model that can predict which construction will be used given a set of contextual features. In the test case these features include GENRE, ANIMACY, NOUN TYPE (proper vs. common), LENGTH (in words), THEMATICITY (text frequency) of the possessor, and FINAL SIBILANT (whether the possessor phrase ends in a sibilant sound).

## Fit RF model with `{ranger}`

First we'll convert the relevant columns to factors.

```{r}
written_genitives <- written_genitives |>
  mutate(
    Type = as.factor(Type),
    Response = as.numeric(Type) - 1,
    Genre = as.factor(Genre),
    Possessor_Animacy3 = as.factor(Possessor_Animacy3),
    Final_Sibilant = as.factor(Final_Sibilant),
    Possessor_NP_Type = as.factor(Possessor_NP_Type)
  )
```

Define the formula using some contextual features known to correlate with the use of genitive constructions.

```{r}
fmla <- Type ~ Genre + Possessor_Animacy3 + Possessor_NP_Type +
  Possessor_Length + Possessor_Thematicity + Final_Sibilant

# Check that we have factors (not character vectors)
glimpse(written_genitives[, all.vars(fmla)])
```

Fit the model with `ranger()`. For the `{predictiveMargins}` package **we must use a probability forest rather than a classification forest** by setting `probability = TRUE`. See `?ranger` for more details. In a real case we would also tune the model's hyperparameters, e.g. with the `{tidymodels}` or `{tuneRanger}` packages. But this will only affect the performance of the model, and does not affect the functions in `{predictiveMargins}`, so we won't bother with it here. 

```{r ranger-rf}
library(ranger)

rf_ranger <- ranger(
  fmla,
  data = written_genitives,
  num.trees = 1000,
  mtry = 3,
  probability = TRUE,
  respect.unordered.factors = "partition"
)
rf_ranger
```


### Evaluate model

***Before exploring the forest predictions, it's necessary to evaluate the model. Results derived from a poorly performing model are not reliable, so this is always a necessary first step.*** 

The `{caret}` package provides some good diagnostic tools. First we create a dataframe of the predicted probabilities and the observed vs. predicted outcome. 

```{r}
rf_ranger_preds <- rf_ranger$predictions |> 
  as.data.frame() |> 
  dplyr::mutate(
    obs = written_genitives$Type,
    pred = as.factor(if_else(of > .5, "of", "s"))
  )
head(rf_ranger_preds)
```

Use the `twoClassSummary()` function to get a quick assessment of the performance of the RF model. Here we focus on the area under the Receiver Operating Characteristic curve (`ROC`), which is a reasonable measure of how well the model discriminates between two outcomes. The scores range from .5 (chance) to 1 (perfect discrimination). Values above .8 are considered good performance. The discriminatory ability of the current RF model is .912. The interpretation of this measure is fairly straightforward: If we pick a true *s*-genitive and a true *of*-genitive at random from the data, there is a 91% chance that our RF model gives the true *s*-genitive a higher probability of being an *s*-genitive.

```{r}
caret::twoClassSummary(
  rf_ranger_preds,
  lev = levels(rf_ranger_preds$obs)
) |> 
  round(3)
```

The confusion matrix gives even more details.

```{r}
caret::confusionMatrix(
  data = rf_ranger_preds$pred,
  reference = rf_ranger_preds$obs,
  mode = "everything"
)
```

Our model is a pretty good one. More information on these measures can be found on the `{caret}` documentation here: [https://topepo.github.io/caret/measuring-performance.html](https://topepo.github.io/caret/measuring-performance.html)

## Fit RF model with `{party}`

Now we try a forest with the `{party}` package. Again, in a real case we would tune the `mtry` and other hyperparameters. One disadvantage is that `{party}` forests run *much* more slowly than `{ranger}`.

```{r party-rf, cache=TRUE}
library(party)

rf_party <- cforest(
  fmla,
  data = written_genitives,
  control = cforest_control(ntree = 1000L, mtry = 3)
)
rf_party
```

### Evaluate model

Once again, we must evaluate the model. We can get the class-specific predicted probabilities with `treeresponse()` and then bind them into a dataframe. This also can take a long time, so we won't run it in this vingette, but the code is included below for illustration. Generally, the `{party}` forests perform *slightly* better than `{ranger}` ones in our experience, although this small improvement may not be worth the added computation time.

```{r party-rf-preds, eval=FALSE}
# NOT RUN
rf_party_preds <- do.call("rbind", treeresponse(rf_party)) 
```

Create a similar data.frame of predictions and observations as above and then use the same methods from `{caret}`.

```{r eval=FALSE}
# NOT RUN
rf_party_preds <- rf_party_preds |> 
  as.data.frame() |> 
  rename("of" = "Type.of", "s" = "Type.s") |> 
  dplyr::mutate(
    obs = written_genitives$Type,
    pred = as.factor(if_else(of > .5, "of", "s"))
  )
head(rf_party_preds)
```

Use the `twoClassSummary()` function to get a quick assessment of the performance of the RF model. 

```{r eval=FALSE}
# NOT RUN
caret::twoClassSummary(
  rf_party_preds,
  lev = levels(rf_party_preds$obs)
) |> 
  round(3)
```

The confusion matrix.

```{r eval=FALSE}
# NOT RUN
caret::confusionMatrix(
  data = rf_party_preds$pred,
  reference = rf_party_preds$obs,
  mode = "everything"
)
```


## Get predicted probabilities

To get the predicted probabilities of the trees, we use the `tree_predictions()` function. The two obligatory arguments are the model, e.g. `rf_ranger`, and the dataset used to train the model, `written_genitives`. The third argument `breaks = list(Possessor_Length = c(1, 2, 3, 4))` tells the function to calculate predictions only for values of `Possessor_Length` at 1, 2, 3, and 4. For any other continuous predictor variables it will use *n* evenly spaced points (10 by default) spanning the range of the variable. If `breaks` is not specified, all continuous predictors will use the same number of points, set by `n.breaks`.

Additional optional arguments include:

- `num.trees`: the number of trees from which to take predictions. If this number is smaller than the total number of trees in the forest, a random sample is taken. The default is 500 trees. This is to keep the resulting datasets to a reasonable size, as forests may contain many thousands of trees.
- `n.breaks`: the number of breakpoints for binning continuous predictor variables that are not otherwise specified in `breaks`. Default is 10. The values used for prediction are the midpoints of the resulting bins. *Note that increasing this number can greatly impact speed and memory cost, especially when there are many other predictor variables.*
- `verbose`: If `TRUE` additional information will be printed about the dataset.

```{r tree-preds-ranger}
tree_preds_ranger <- tree_predictions(
  rf_ranger, 
  written_genitives, 
  breaks = list(Possessor_Length = c(1, 2, 3, 4)) # only calculate predictions for these values
  )
```

The resulting object `tree_preds_ranger` is a list of class `"treePredictions"`. A dataframe with the predicted probabilities is contained in `tree_preds_ranger$predictions`.

```{r}
head(tree_preds_ranger$predictions)
```


Now try the same with `rf_party`. As always with this package, it takes longer to run...

```{r tree-preds-party}
tree_preds_party <- tree_predictions(
  rf_party, 
  written_genitives, 
  breaks = list(Possessor_Length = c(1, 2, 3, 4)), # only calculate predictions for these values
  verbose = FALSE
  )
head(tree_preds_party$predictions)
```

See `?tree_predictions` for more information.


### Getting average predictions for a target predictor

Now we can summarize the predictions. First we'll calculate average predictions for the target variable `Possessor_Animacy3`. All other predictors in the model (i.e. `Genre`, `Possessor_Animacy3`, `Possessor_NP_Type`, `Possessor_Length` `Possessor_Thematicity`, `Final_Sibilant`) are therefore 'peripheral' variables, which means that we will average over their levels/values. By default, weighted averages are used for all peripheral variables. The effect of using weighted averages is to give more importance to more likely/frequent combinations of predictor values, and less importance to rare and/or very unlikely ones. 

In a nutshell, we propose two possible weighting schemes based on the distributions of predictor variables in the data. We can consider these distributions independently, or in combination with each other. To illustrate, consider the contingency table showing the distribution of `Possessor_NP_Type` and `Possessor_Animacy3` values in the dataset. 

```{r}
xtabs(~ Possessor_NP_Type + Possessor_Animacy3, written_genitives) |> 
  addmargins()
```

Counting down the row sums, the marginal probabilities for values of `Possessor_NP_Type` are:

- *P*(common): 3307/5098 = `r round(3307/5098, 3)`
- *P*(proper): 1791/5098 = `r round(1791/5098, 3)`

Likewise, counting across the column sums, the marginal probabilities for values of `Possessor_Animacy3` are:

- *P*(animate): 2043/5098 = `r round(2043/5098, 3)`
- *P*(collective): 690/5098 = `r round(690/5098, 3)`
- *P*(inanimate): 2365/5098 = `r round(2365/5098, 3)`

Based on these we can calculate the **expected** joint probability of each combination of NP type and animacy by simply multiplying the (independent) marginal probabilities. So, e.g. the expected probability of an inanimate proper noun possessor would be .464 * .351 = `r round((1791/5098)*(2365/5098), 3)`. When averaging the forest predictions across combinations of these two predictors, `r round((1791/5098)*(2365/5098), 3)` would be our weighting for this combination of predictor values. We refer to this scheme as 'isolated' weighting (`"iso"`), since distributions are considered in isolation from one another.

On the other hand, we could consider weighting by the **observed** proportionsof specific combinations in our data. In this case we simply divide the observed counts in each cell by the total number of observations. So the observed proportion of inanimate proper noun possessors is 890/5098 = `r round(368/5098, 3)`. This value is our weighting for this combination of predictor values in the alternative scheme. We refer to this scheme as 'joint' weighting (`"joint"`). 

Note that these different weighting schemes have slightly different interpretations, and may yield very different results. Examination of our data suggests that the differences are minimal, however, more testing is needed to understand the full implications of using these weigthing schemes in various contexts. See Sönning & Grafmiller (2022: 8-13) for further discussion. 


#### Average predictions for a categorical predictor

The function `avg_predictions()` takes two arguments, the output of `tree_predictions()` and a character vector of the names of the target variable(s) (`target.vars = "..."`) in the model for which to obtain the average predictions. Additional optional arguments include:

- `equal.wt`: Character string of predictor variable names across which equal weighting (i.e. simple averages) will be applied.
- `wt`: Character string indicating the type of weighting---'isolated' (`"iso"`), or 'joint' (`"joint"`)---to be applied to those peripheral predictors **not** specified in `equal.wt`. See Sönning & Grafmiller (2022) for discussion of the weighting schemes. 
- `interval`: The lower and upper bounds of the percentile interval summarizing the central distribution of predicted probabilities across the trees in the forest. Default is `c(.05, .95)`, for a 90% interval.
- `verbose`: If `TRUE` additional information will be printed about the dataset.

The default weighting is the isolated weighting (`"iso"`), where the weights are based on the expected distribution of all combinations of the peripheral variables (i.e. those **not** `Possessor_Animacy3` and **not** specified in the `equal.wt` argument), calculated from the independent marginal (overall) distributions of each peripheral predictor in the dataset.

```{r anim-avg-ranger1}
# messages can be turned off with `verbose = FALSE`
animacy_avg_iso <- avg_predictions(
  tree_preds_ranger, 
  target.vars = "Possessor_Animacy3"
  )
animacy_avg_iso
```

Now try the joint weighting. Again, this weighting uses the observed numbers of each combination in the original training dataset.

```{r anim-avg-ranger2}
animacy_avg_joint <- avg_predictions(tree_preds_ranger, "Possessor_Animacy3", wt = "joint")
```

We can plot these weightings and compare. Here, the differences are negligible, but they can vary (considerably) in some cases.

```{r}
bind_rows(
  animacy_avg_iso |>
    mutate(weighting = "isolated"),
  animacy_avg_joint |>
    mutate(weighting = "joint")
) |>
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_pred, color = weighting)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = weighting, linetype = weighting), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper, shape = weighting),
                  position = position_dodge(width = .2)) +
  theme_classic()
```

We can also compare the different random forest methods (`{ranger}` vs. `{party}`) by calculating the predictions for the two methods.

```{r anim-avg-party1}
animacy_avg_iso_party <- avg_predictions(
  tree_preds_party, 
  target.vars = "Possessor_Animacy3",
  verbose = FALSE
  )
```

Plot the results to compare.

```{r}
bind_rows(
  animacy_avg_iso |>
    mutate(method = "ranger"),
  animacy_avg_iso_party |>
    mutate(method = "party")
) |>
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_pred, color = method)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = method, linetype = method), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper, shape = method),
                  position = position_dodge(width = .2)) +
  theme_classic()
```

#### Average predictions for a continuous predictor

Calculating predictions for continuous predictors is easy as well.

```{r them-avg-party1}
# use party model
avg_predictions(
  tree_preds_party, "Possessor_Thematicity", wt = "iso", verbose = F
  ) |>
  ggplot(aes(Possessor_Thematicity, mean_s_pred)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic() + ggtitle("Thematicity in {party} model")
```

For `Possessor_Length` recall that we only calculated predictions for values of 1 through 4. 

```{r them-avg-ranger1}
avg_predictions(tree_preds_party, "Possessor_Length", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Length, mean_s_pred)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic() + ggtitle("Possessor Length in {party} model")
```

If we want more values for this predictor, we'll need to recalculate the tree predictions, setting the `breaks` argument to a different set of numbers.
 
```{r tree-preds-party2}
tree_preds_party2 <- tree_predictions(
  rf_party, 
  written_genitives, 
  breaks = list(Possessor_Length = 1:8) # only calculate predictions for these values
  )
```
 
Now re-generate the plot.
 
```{r}
avg_predictions(tree_preds_party2, "Possessor_Length", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Length, mean_s_pred)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) + 
  scale_x_continuous(breaks = 1:8) + # put the ticks at integers
  theme_classic()
```

We can also examine more fine-grained values of `Possessor_Thematicity`, e.g. by setting the `n.breaks` to 30, since we are not necessarily interested in specific values of this predictor. *Note again that increasing this number can greatly impact speed and memory cost, especially when there are many other predictor variables. If there are many continuous predictors, we recommend keeping `n.breaks` small and setting the values for specific predictors inside `breaks`.*

```{r tree-preds-party3}
tree_preds_party3 <- tree_predictions(
  rf_party, 
  written_genitives, 
  n.breaks = 30,
  breaks = list(Possessor_Length = 1:8) # only calculate predictions for these values
  )
```

Now plot the new results.

```{r}
avg_predictions(tree_preds_party3, "Possessor_Thematicity", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Thematicity, mean_s_pred)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic()
```

The overall picture is not much different, but the resolution is finer. Multiple numeric predictors can be specified in the `breaks` argument.

```{r eval=F}
# NOT RUN
# use the 10% quantile values for Possessor_Thematicity
quantiles <- quantile(written_genitives$Possessor_Thematicity, seq(0, 1, .1))
tree_preds_party3 <- tree_predictions(
  rf_party, 
  written_genitives, 
  breaks = list(Possessor_Length = 1:8, Possessor_Thematicity = quantiles) 
  )
```


### Excluding variables from the weighting

Sönning & Grafmiller (2022) also discuss the possibility of excluding certain peripheral variables from the weighting. This amounts to weighting predictions equally across all values of these 'external' predictors when computing the average predictions.

Here we exclude `Genre` from the weighting in the `{party}` model.

```{r}
avg_predictions(tree_preds_party, "Possessor_Animacy3", equal.wt = "Genre", verbose = FALSE) |> 
  dplyr::arrange(Possessor_Animacy3)
```

Compare this to the case where `Genre` is included in the weighting. 

```{r}
animacy_avg_iso_party |> 
  dplyr::arrange(Possessor_Animacy3)
```

Here again we see that the difference is very minor.


### Interaction effects

Multivariate associations among predictor variables and the outcome (i.e. "interaction effects") can be calculated by simply including multiple names in the vector of target variables.

```{r}
# use the ranger model
avg_predictions(tree_preds_ranger, c("Possessor_Animacy3", "Possessor_NP_Type"), verbose = F) |> 
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_pred, color = Possessor_NP_Type)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = Possessor_NP_Type), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper),
                  position = position_dodge(width = .2)) +
  ggtitle("s-genitive probability by\nPossessor Animacy and NP type") +
  theme_classic()
```

Three-way (and more) patterns are possible with this method. For example, the 3-way interaction of `Possessor_Animacy3`, `Possessor_NP_Type`, and `Genre`:

```{r fig.width=6.5, fig.height=5}
avg_predictions(tree_preds_ranger, c("Possessor_Animacy3", "Possessor_NP_Type", "Genre"), 
             verbose = F) |> 
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_pred, color = Possessor_NP_Type)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = Possessor_NP_Type), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper),
                  position = position_dodge(width = .2)) + 
  ggtitle("s-genitive probability by\nPossessor Animacy, NP type, and Genre") +
  facet_wrap(~ Genre) + 
  theme_classic() +
  theme(legend.position = "bottom") 
```

Interactions are just as easy with continuous predictors.

```{r fig.width=7}
avg_predictions(
  tree_preds_party2, 
  target.vars = c("Possessor_Length", "Genre"), 
  verbose = F
  ) |>
  ggplot(aes(Possessor_Length, mean_s_pred)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) + 
  facet_wrap(~Genre) +
  ggtitle("S-genitive probability by\nPossessor Length and Genre") +
  scale_x_continuous(breaks = 1:8) + # put the ticks at integers
  theme_classic()
```

Plotting interactions of 2 or more continuous predictors is not as simple, but the calculations are straightforward with `avg_predictions()`. Plotting the interaction of `Possessor_Length` and `Possessor_Thematicity`, for instance, suggests that their effects are largely independent of one another. 

```{r fig.width=7}
avg_predictions(
  tree_preds_party2, 
  target.vars = c("Possessor_Length", "Possessor_Thematicity"), 
  verbose = F
  ) |>
  ggplot(aes(Possessor_Length, Possessor_Thematicity)) +
  geom_raster(aes(fill = mean_s_pred)) +
  ggtitle("S-genitive probability by\nPossessor Length and Thematicity") +
  scale_x_continuous(breaks = 1:8) + # put the ticks at integers
  theme_classic()
```

\_

## Predictive contrasts

We refer to "predictive contrasts" as the difference between two predicted outcomes for specific values (levels) of a predictor. Summarizing over these contrasts is one way we can get a measure of the effect size and direction of a given target predictor, similar in spirit to a regression coefficient (at least for categorical predictors).

For example, for the targeted variable `Possessor_NP_Type`, which has two levels (common vs. proper nouns), the predictive contrast is the difference between the predicted probabilities for the two levels. For instance, if the predicted probabilities are .55 for common nouns and .70 for proper nouns, the predicted contrast (proper -- common) is +.15. These contrasts are estimated for every combination of predictor values, averaged over all the trees in the forest. 

### Predictive contrasts for binary variables

For binary targeted variables, which level is subtracted from which depends on the coding of the factor. By default, R orders factor levels alphabetically. For `Possessor_NP_Type`, `common` is therefore represented (internally) as `1`, and `proper` as `2`. For binary predictors, the first level (`common`) is subtracted from the second (`proper`). *Note that the function will print a message stating which level is subtracted from which.*

The function `predicted_contrasts()` takes three arguments, the output of `tree_predictions()`, a character vector of the names of the targeted variable in the model for which to obtain predicted contrasts (`target.vars = "..."`), and an `interval` argument (default is 90%).

The output of `predicted_contrasts()` is a data frame, with a column reporting the obtained difference in predicted probabilities (i.e. the predicted contrasts) for the target predictor (here `proper_common_contrast`), and columns  `lower` and `upper` reflecting the bounds of the percentile interval. In this case, contrasts represent the difference: `Possessor_NP_Type == 'proper'` -- `Possessor_NP_Type == 'common'`. 

```{r}
# Use ranger model
np_type_contrasts <- predicted_contrasts(tree_preds_ranger, "Possessor_NP_Type")
head(np_type_contrasts)
```

Plot the distribution of the combination-specific contrasts in predicted outcome.

```{r}
# calculate the median contrast
median_cont <- median(np_type_contrasts$proper_common_contrast)
# histogram of contrasts for all combinations of predictors
ggplot(np_type_contrasts, aes(x = proper_common_contrast)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_histogram(color = "black", fill = "gray", bins = 50) +
  geom_vline(xintercept = median_cont, color = "red", size = 1) +
  annotate(geom = "text", x = median_cont + .05, y = 80, 
           color = "red", hjust = 0,
           label = paste("median =", round(median_cont, 3))) +
  labs(x = "Difference in predicted s-genitive probability\n(proper - common)",
       y = "") +
  theme_classic() + ggtitle("NP type contrasts in {ranger} model")
```

Try with `{party}` model.

```{r}
np_type_contrasts_party <- predicted_contrasts(tree_preds_party, "Possessor_NP_Type")
median_cont <- median(np_type_contrasts_party$proper_common_contrast)
ggplot(np_type_contrasts_party, aes(x = proper_common_contrast)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_histogram(color = "black", fill = "gray", bins = 50) +
  geom_vline(xintercept = median_cont, color = "red", size = 1) +
  annotate(geom = "text", x = median_cont + .05, y = 80, 
           color = "red", hjust = 0,
           label = paste("median =", round(median_cont, 3))) +
    labs(x = "Difference in predicted s-genitive probability\n(proper - common)",
       y = "") +
  theme_classic() + ggtitle("NP type contrasts in {party} model")
```

### Average contrasts

Weighted averages for contrasts can also be computed with `avg_contrasts()`. This is useful for summarizing the patterns over different levels of secondary predictors. Fo example we calculate the weighted average contrasts for `Possessor_NP_Type` by `Genre` like so.

```{r}
avg_contrasts(tree_preds_ranger, "Possessor_NP_Type", by = "Genre", verbose = F)
```

Multiple secondary predictors can be included in the `by =` argument.

```{r avg-contrast-ranger2}
avg_contrasts(tree_preds_ranger, "Possessor_NP_Type", by = c("Genre", "Possessor_Animacy3" ), 
              wt = "joint", verbose = F)
```


### Predictive contrasts for categorical variable with +3 levels

The calculation of predictive contrasts for categorical predictors with more than two levels works similarly. The output of `predicted_contrasts()` is again a data frame, but now there is a set of columns, one for each contrast. The targeted variable `Possessor_Animacy3`, for instance, has 3 levels, so there are three possible contrasts. Accordingly, the data frame includes three contrast columns: `contrast_animate_collective`, `contrast_animate_inanimate`, and `contrast_collective_inanimate`. Again, the naming is such that for `contrast_animate_collective`, the prediction for `collective` is subtracted from that for `animate`. A predicted contrast of +.20 therefore signals that the predicted probability (of *s*-genitives) is higher by .2 for `animate` compared to `collective` referents.

```{r}
animacy_contrasts <- predicted_contrasts(tree_preds_ranger, "Possessor_Animacy3")
animacy_contrasts |> 
  summarise(across(6:8, ~ median(.x)))
```

We can visualize the results.

```{r fig.width=6}
library(ggridges)

animacy_contrasts |> 
  pivot_longer(6:8, names_to = "contrast") |> 
  ggplot(aes(value, contrast)) +
  geom_density_ridges() +
  theme_classic() +
  labs(y = "", x = "Difference in predicted s-genitive probability")
```

Basically, for a binary outcome and a predictor of $n$ levels, there are $k$ possible contrasts, where $k$ is calculated as

$$ k = \frac{n!}{2(n - 2)!}$$

So for `Genre`, which has 5 levels, there are 10 possible contrasts.

```{r}
genre_contrasts <- predicted_contrasts(tree_preds_ranger, "Genre")
glimpse(genre_contrasts)
```

We can plot these and color by the comparisons with Press (previous work has shown a distinct difference between Press and other genres/registers, so this is a potentially interesting comparison).

```{r fig.width=7}
genre_contrasts |> 
  pivot_longer(6:15, names_to = "contrast") |> 
  mutate(
    Is.Press = if_else(grepl("Press", contrast), "press", "nonpress")
  ) |> 
  ggplot(aes(value, contrast, fill = Is.Press)) +
  geom_density_ridges() +
  # stat_summary(geom = "pointrange", fun.data = "median_hilow") +
  theme_classic() +
  labs(y = "", x = "Difference in predicted s-genitive probability")
```

All the comparisons with Press show long left tails. Because these reflect the difference in predicted probability of GENRE - Press, the left tails tell us that there are many combinations in which the *s*-genitive is predicted to be **less** likely in the stated genre than in Press texts.   

We can compare this to the average predictions calculated with `avg_predictions()`, where it's clear that the "Press" genre stands out from the rest.

```{r}
avg_predictions(
  tree_preds_ranger, 
  target.vars = "Genre",
  verbose = FALSE
  )
```

# Predicting continuous outcomes (regression)

The package can be used for cases with continuous outcomes. 

To illustrate, we use the `english` dataset of response times for a lexical decision task, included in the `{languageR}` package (Baayen & Shafaei-Bajestan 2019). This gives mean visual lexical decision latencies and word naming latencies for 2284 monomorphemic English nouns and verbs, averaged for old and young participants, with various other predictor variables associated with response times. You can find more information about this dataset by consulting the documentation with `?languageR::english`. 

```{r}
english_rt <- languageR::english
head(english_rt)
```

So here we are interested, e.g., in the association of things like word FAMILIARITY and subject AGE with response times for identifying words in a timed decision task. 

```{r}
english_rt |> 
  ggplot(aes(Familiarity, RTlexdec)) +
  geom_text(aes(label = Word, color = AgeSubject), size = 3) +
  theme_classic() +
  ggtitle("Responses are quicker for more familiar words and\nyounger participants")
```

The outcome in our model will be the mean response time latency `RTlexdec` for the words in the dataset.

## Fit RF models

As above, we'll illustrate with both `{ranger}` and `{party}` methods.

A `{ranger}` forest.

```{r eng-forest-ranger, cache=TRUE}
rf_english_ranger <- ranger(
  RTlexdec ~ Familiarity + AgeSubject + WordCategory + WrittenFrequency +
    FamilySize + LengthInLetters,
  english_rt,
  mtry = 3,
  num.trees = 500,
  respect.unordered.factors = "partition"
)
```

A `{party}` forest.

```{r eng-forest-party, cache=TRUE}
rf_english_party <- cforest(
  RTlexdec ~ Familiarity + AgeSubject + WordCategory + WrittenFrequency +
    FamilySize + LengthInLetters,
  data = english_rt,
  control = cforest_control(mtry = 3, ntree = 500L)
)
```

## Get predicted response times

Now get the tree predictions for the forests. 

```{r eng-tree-preds, cache=TRUE}
eng_preds_ranger <- tree_predictions(rf_english_ranger, english_rt, verbose = F)
eng_preds_party <- tree_predictions(rf_english_party, english_rt, verbose = F)
```

Again, the predicted outcome is the response latency for the experimental task, `RTlexdec_pred`.

```{r}
head(eng_preds_party$predictions)
```


### Average predictions for target predictors

Calculate the average predicted response latency by age of subject.

```{r avg-preds-ranger1, cache=TRUE}
avg_predictions(eng_preds_ranger, "AgeSubject", wt = "iso")
```

Calculate the average predicted response latency by age of subject and word familiarity.

```{r}
fam_age_preds <- avg_predictions(eng_preds_ranger, c("AgeSubject", "Familiarity"), 
                                 verbose = F) 
```


We can plot the averages overlaying the predicted trend lines on top of the individual observations (the points).

```{r fig.width=7}
fam_age_preds |> 
  ggplot(aes(Familiarity, mean_RTlexdec_pred, color = AgeSubject)) +
  geom_point(data = english_rt, aes(y = RTlexdec), alpha = .2) +
  geom_line(aes(group = AgeSubject), size = 6, color = "white") +
  geom_line() +
  geom_point(aes(shape = AgeSubject), size = 3) +
  theme_classic() +
  ggtitle("Predicted respone time latencies by Age and Word Familiarity",
          subtitle = "Dots represent individual word means")
```


## Predictive contrasts

Calculate the predicted contrasts for `AgeSubject`.

```{r age-contrast-ranger, cache=TRUE}
age_contrasts <- predicted_contrasts(eng_preds_ranger, "AgeSubject")
head(age_contrasts)
```

Again, we can plot the distribution of contrasts

```{r}
median_cont <- median(age_contrasts$young_old_contrast)
age_contrasts |> 
  ggplot(aes(young_old_contrast)) +
  geom_vline(xintercept = 0, color = "grey", size = 2) +
  geom_histogram(color = "black", fill = "gray", bins = 50) +
  geom_vline(xintercept = median_cont, color = "red", size = 1) +
  annotate(geom = "text", x = median_cont + .03, y = 1250, 
           color = "red", hjust = 0,
           label = paste("median =", round(median_cont, 3))) +
  labs(x = "Difference in predicted response latency (young - old)", y = "") +
  theme_classic()
```

There is good evidence of a strong and reliable difference in response times across age groups, as the distribution is almost entirely well below 0.   


# References

Baayen RH, Shafaei-Bajestan, E. 2019. *languageR: Analyzing Linguistic Data: A Practical Introduction to Statistics*. R package version 1.5.0, [https://CRAN.R-project.org/package=languageR](https://CRAN.R-project.org/package=languageR).

Sönning, Lukas & Jason Grafmiller. 2022. Seeing the wood for the trees: Predictive margins for random forests. Preprint. *PsyArXiv*. [https://doi.org/10.31234/osf.io/jr8yk](https://doi.org/10.31234/osf.io/jr8yk).








