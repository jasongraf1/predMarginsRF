---
title: "Using predictive margins"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette 
vignette: >
  %\VignetteIndexEntry{Using predictive margins}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 4,
  fig.width = 5
)
```

This vignette demonstrates how to use the core functions in the `{predictiveMargins}` package. For details of the method and theoretical motivation see Sönning & Grafmiller (2022), [Seeing the wood for the trees: Predictive margins for random forests](https://doi.org/10.31234/osf.io/jr8yk).

## Libraries

For this vignette we'll the following basic libraries. 

```{r setup}
library(tidyverse) # for  data wrangling and plotting
library(data.table) # for much faster data wrangling and computation
```

Install the `{predictiveMargins}` package.

```{r eval = FALSE}
remotes::install_github("jasongraf1/predictiveMargins")
```

Load the library as usual.

```{r}
library(predictiveMargins)
```


## Data

For this tutorial, we'll use the `written_genitives` dataset of English genitive constructions (*the children's voices* [*s*-genitive] vs. *the voices of the children* [*of*-genitive]) included in the package and discussed Sönning & Grafmiller (2022). See `?written_genitives` for more on the dataset.

```{r}
glimpse(written_genitives)
```

We're interested in exploring the featues that correlate with use of different constructions. We know, for instance, that animate (i.e. human) possessors tend to co-occur more often with the *s*-genitive, and inanimate possessors more with the *of*-genitive.

```{r}
ggplot(written_genitives, aes(Possessor_Animacy3, fill = Type)) +
  geom_bar(position = "fill") +
  labs(y = "Proportion of observed genitive constructions",
       title = "Genitives by possessor animacy",
       subtitle = "In 1960s and 1990s written American English") +
  theme_classic()
```
So the aim is to build a model that can predict which construction will be used given a set of contextual features.

## Fit RF model with `{ranger}`

First we'll convert the relevant columns to factors.

```{r}
written_genitives <- written_genitives |>
  mutate(
    Type = as.factor(Type),
    Response = as.numeric(Type) - 1,
    Genre = as.factor(Genre),
    Possessor_Animacy3 = as.factor(Possessor_Animacy3),
    Final_Sibilant = as.factor(Final_Sibilant),
    Possessor_NP_Type = as.factor(Possessor_NP_Type)
  )
glimpse(written_genitives)
```


Define the formula using some contextual features known to correlate with the use of genitive constructions, e.g. the genre, the animacy, noun type (proper vs. common), length (in words), frequency ('thematicity') of the possessor, and whether the possessor phrase ends in a sibilant sound.

```{r}
fmla <- Type ~ Genre + Possessor_Animacy3 + Possessor_NP_Type +
  Possessor_Length + Possessor_Thematicity + Final_Sibilant
```

Fit the model. Here we use a probability forest rather than a classification forest by setting `probability = TRUE`. In a real case we would also tune the model's hyperparameters, e.g. with the `{tidymodels}` or `{tuneRanger}` packages. But this will only affect the performance of the model, and does not affect the functions in `{predictiveMargins}`, so we don't bother here.

```{r ranger-rf}
library(ranger)

rf_ranger <- ranger(
  fmla,
  data = written_genitives,
  num.trees = 1000,
  mtry = 3,
  probability = TRUE,
  respect.unordered.factors = "partition"
)
rf_ranger
```


### Evaluate model

Before generating the predictive margins, it's necessary to evaluate the model. Results derived from a poorly performing model are not reliable, so this is always a necessary first step. 

The `{caret}` package provides some good diagnostic tools. First we create a dataframe of the predicted probabilities and the observed vs. predicted outcome. 

```{r}
rf_ranger_preds <- rf_ranger$predictions |> 
  as.data.frame() |> 
  dplyr::mutate(
    obs = written_genitives$Type,
    pred = as.factor(if_else(of > .5, "of", "s"))
  )
head(rf_ranger_preds)
```

Use the `twoClassSummary()` function to get a quick assessment of the performance of the RF model. Here we focus on the area under the Receiver Operating Characteristic curve (`ROC`), which is a reasonable measure of how well the model discriminates between two outcomes. The scores range from .5 (chance) to 1 (perfect discrimination). Values above .8 are considered good performance.

```{r}
caret::twoClassSummary(
  rf_ranger_preds,
  lev = levels(rf_ranger_preds$obs)
) |> 
  round(3)
```

The confusion matrix gives even more details.

```{r}
caret::confusionMatrix(
  data = rf_ranger_preds$pred,
  reference = rf_ranger_preds$obs,
  mode = "everything"
)
```

Our model is a pretty good one (more information on these measures can be found on the `{caret}` documentation here: [https://topepo.github.io/caret/measuring-performance.html](https://topepo.github.io/caret/measuring-performance.html).

## Fit model with `{party}`

Now we try with the `{party}` package. Again, in a real case we would tune the `mtry` and other hyperparameters. Note that `{party}` forests run much more slowly than `{ranger}`.

```{r party-rf, cache=TRUE}
library(party)

rf_party <- cforest(
  fmla,
  data = written_genitives,
  control = cforest_control(ntree = 1000L, mtry = 3)
)
rf_party
```

### Evaluate model

Once again, evaluate the model. We can get the class-specific predicted probabilities with `treeresponse()` and then bind them into a dataframe. This also takes time, so we won't run it here. But the code is included below for illustration.

```{r party-rf-preds, eval=FALSE}
# not run
rf_party_preds <- do.call("rbind", treeresponse(rf_party)) 
```

Create a similar data.frame of predictions and observations as above and then use the same methods from `{caret}`.

```{r eval=FALSE}
# not run
rf_party_preds <- rf_party_preds |> 
  as.data.frame() |> 
  rename("of" = "Type.of", "s" = "Type.s") |> 
  dplyr::mutate(
    obs = written_genitives$Type,
    pred = as.factor(if_else(of > .5, "of", "s"))
  )
head(rf_party_preds)
```

Use the `twoClassSummary()` function to get a quick assessment of the performance of the RF model. 

```{r eval=FALSE}
# not run
caret::twoClassSummary(
  rf_party_preds,
  lev = levels(rf_party_preds$obs)
) |> 
  round(3)
```

The confusion matrix.

```{r eval=FALSE}
# not run
caret::confusionMatrix(
  data = rf_party_preds$pred,
  reference = rf_party_preds$obs,
  mode = "everything"
)
```

## Get marginal predictions

To get the marginal predictions of the trees, we use the `marginal_predictions()` function. The two obligatory arguments are the model `rf_ranger` and the dataset used to train the model, `written_genitives`. The third argument `breaks = list(Possessor_Length = c(1, 2, 3, 4))` tells the function to calculate predictions only for values of `Possessor_Length` at 1, 2, 3, and 4. For any other continuous predictor variables it will use *n* evenly spaced points (10 by default) spanning the range of the variable. If `breaks` is not specified, all continuous predictors will use the same number of points, set by `n.breaks`.

Additional optional arguments include:

- `num.trees`: the number of trees from which to take predictions. If this number is smaller than the total number of trees in the forest, a random sample is taken. The default is 500 trees. This is to keep the resulting datasets to a reasonable size, as forests may contain many thousands of trees.
- `n.breaks`: the number of breakpoints for binning continuous predictor variables that are not otherwise specified in `breaks`. Default is 10. The values used for prediction are the midpoints of the resulting bins. *Note that increasing this number can greatly impact speed and memory cost, especially when there are many other predictor variables.*
- `verbose`: If `TRUE` additional information will be printed about the dataset.

```{r}
marginal_preds <- marginal_predictions(
  rf_ranger, 
  written_genitives, 
  breaks = list(Possessor_Length = c(1, 2, 3, 4)) # only calculate predictions for these values
  )
```

`marginal_preds` is a list. The prediction dataframe is contained in `marginal_preds$predictions`.

```{r}
head(marginal_preds$predictions)
```


Now try the same with `rf_party`. As always with this package, it takes longer to run...

```{r}
marginal_preds_party <- marginal_predictions(
  rf_party, 
  written_genitives, 
  breaks = list(Possessor_Length = c(1, 2, 3, 4)) # only calculate predictions for these values
  )

head(marginal_preds_party$predictions)
```


See `help("marginal_predictions")` for more information.





### Average marginal predictions for a categorical predictor

First we'll calculate the weighted average marginal predictions for the association of the target variable `Possessor_Animacy3` with genitive construction. See Sönning & Grafmiller (2022) for full discussion of the weighting schemes used.


The function `marginal_avg()` takes two arguments, the model and a character vector of the names of the target variables in the model for which to average the predictions. Additional optional arguments include:

- `ext.vars`: Character string of external predictor variable names across which equal weighting will be applied.
- `wt`: Character string indicating the type of weighting to be applied to the calculation of mean predictions. - `interval`: The lower and upper quantiles for the distribution of predicted probabilities. Default is `c(.05, .95)`
- `verbose`: If `TRUE` additional information will be printed about the dataset.

The default weighting is the isolated weighting (`"iso"`), where the weights are based on the expected distributions of all combinations of the peripheral variables (i.e. those **not** `Possessor_Animacy3`), calculated from the independent marginal distributions of each peripheral predictor.

```{r}
animacy_mar_avg_iso <- marginal_avg(
  marginal_preds, 
  target.vars = "Possessor_Animacy3"
  )
animacy_mar_avg_iso
```

Now try the joint weighting.

```{r}
animacy_mar_avg_joint <- marginal_avg(marginal_preds, "Possessor_Animacy3", wt = "joint")
```

We can plot these and compare. The differences are often negligible, but may vary considerably in some cases.

```{r}
bind_rows(
  animacy_mar_avg_iso |>
    mutate(weighting = "isolated"),
  animacy_mar_avg_joint |>
    mutate(weighting = "joint")
) |>
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_prob, color = weighting)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = weighting, linetype = weighting), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper, shape = weighting),
                  position = position_dodge(width = .2)) +
  theme_classic()
```

We can even compare the different random forest methods by calculating the margins for the two methods.

```{r}
animacy_mar_avg_iso_party <- marginal_avg(
  marginal_preds_party, 
  target.vars = "Possessor_Animacy3"
  )
```

Plot the two results.

```{r}
bind_rows(
  animacy_mar_avg_iso |>
    mutate(method = "ranger"),
  animacy_mar_avg_iso_party |>
    mutate(method = "party")
) |>
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_prob, color = method)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = method, linetype = method), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper, shape = method),
                  position = position_dodge(width = .2)) +
  theme_classic()
```



#### Excluding external variables from the weighting

Sönning & Grafmiller also discuss the possibility of excluding certain peripheral variables from the weighting. This amounts to weighting predictions equally across all values of these 'external' predictors when computing the average predictions.

Here we exclude `Genre` from the weighting in the `{party}` model.

```{r}
marginal_avg(marginal_preds_party, "Possessor_Animacy3", ext.vars = "Genre", verbose = FALSE)
```
Compare this to the case where `Genre` is included in the weighting. 

```{r}
animacy_mar_avg_iso_party
```
Here we see that the difference is very minor.

#### Interaction effects

Multivariate associations among predictor variables and the outcome (i.e. "interaction effects") can be calculated easily by simply including them in the vector of target variables.

```{r}
# use the ranger model
marginal_avg(marginal_preds, c("Possessor_Animacy3", "Possessor_NP_Type"), verbose = F) |> 
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_prob, color = Possessor_NP_Type)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = Possessor_NP_Type), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper),
                  position = position_dodge(width = .2)) +
  ggtitle("s-genitive probability by\nPossessor Animacy and NP type") +
  theme_classic()
```

Three-way (and more) patterns are possible with this method.


```{r fig.width=7}
marginal_avg(marginal_preds, c("Possessor_Animacy3", "Possessor_NP_Type", "Genre"), 
             verbose = F) |> 
  ggplot(aes(x = Possessor_Animacy3, y = mean_s_prob, color = Possessor_NP_Type)) +
  geom_hline(yintercept = .5, color = "grey", linetype = "dashed") +
  geom_line(aes(group = Possessor_NP_Type), position = position_dodge(width = .2)) +
  geom_pointrange(aes(ymin = lower, ymax = upper),
                  position = position_dodge(width = .2)) + 
  ggtitle("s-genitive probability by\nPossessor Animacy, NP type, and Genre") +
  facet_wrap(~ Genre) + 
  theme_classic() +
  theme(legend.position = "bottom") 
```


### Average marginal predictions for continuous predictor

Calculating marginal averages for continuous predictors is easy as well.

```{r}
# use party model
marginal_avg(marginal_preds_party, "Possessor_Thematicity", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Thematicity, mean_s_prob)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic() + ggtitle("Thematicity in {party} model")
```

```{r}
# use ranger model
marginal_avg(marginal_preds, "Possessor_Thematicity", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Thematicity, mean_s_prob)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic() + ggtitle("Thematicity in {ranger} model")
```

For `Possessor_Length` recall that we only calculated predictions for values of 1 through 4. 

```{r}
marginal_avg(marginal_preds, "Possessor_Length", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Length, mean_s_prob)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic()
```

If we want more values for this predictor, we'll need to recalculate the marginal predictions, setting the `break` argument to a different set of numbers.
 
```{r}
marginal_preds2 <- marginal_predictions(
  rf_ranger, 
  written_genitives, 
  breaks = list(Possessor_Length = 1:8) # only calculate predictions for these values
  )
```
 
Now re-generate the plot.
 
```{r}
marginal_avg(marginal_preds2, "Possessor_Length", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Length, mean_s_prob)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) + 
  scale_x_continuous(breaks = 1:8) + # put the ticks at integers
  theme_classic()
```

We can also examine more fine-grained values of `Possessor_Thematicity`, e.g. by setting the `n.breaks` to 30, since we are not necessarily interested in specific values of this predictor. *Note again that increasing this number can greatly impact speed and memory cost, especially when there are many other predictor variables. If there are many continuous predictors, we recommend keeping `n.breaks` small and setting the values for specific predictors inside `breaks`.*

```{r}
marginal_preds3 <- marginal_predictions(
  rf_ranger, 
  written_genitives, 
  n.breaks = 30,
  breaks = list(Possessor_Length = 1:8) # only calculate predictions for these values
  )
```

Now plot the new results.

```{r}
marginal_avg(marginal_preds3, "Possessor_Thematicity", wt = "iso", verbose = F) |>
  ggplot(aes(Possessor_Thematicity, mean_s_prob)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) +
  theme_classic()
```

The overall picture is not much different, but the resolution is finer. Multiple numeric predictors can be specified.

```{r eval=F}
# not run
marginal_preds3 <- marginal_predictions(
  rf_ranger, 
  written_genitives, 
  breaks = list(Possessor_Length = 1:8, Possessor_Thematicity = 30) 
  )
```


#### Interactions

Interactions are just as easy with continuous predictors

```{r fig.width=7}
marginal_avg(
  marginal_preds2, 
  target.vars = c("Possessor_Length", "Genre"), 
  verbose = F
  ) |>
  ggplot(aes(Possessor_Length, mean_s_prob)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) +
  geom_line(aes(group = 1)) + 
  facet_wrap(~Genre) +
  ggtitle("S-genitive probability by\nPossessor Length and Genre") +
  scale_x_continuous(breaks = 1:8) + # put the ticks at integers
  theme_classic()
```


### Contrasts

```{r}
np_type_contrasts <- marginal_contrasts(marginal_preds, "Possessor_NP_Type")
head(np_type_contrasts)
```
Plot the distribution of the combination-specific contrasts in predicted outcome.

```{r}
ggplot(np_type_contrasts, aes(x = mean_Possessor_NP_Type_contrast)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_histogram(color = "black", fill = "gray", bins = 50) +
  geom_vline(xintercept = median(np_type_contrasts$mean_Possessor_NP_Type_contrast), color = "red", size = 1) +
  annotate(geom = "text", x = median(np_type_contrasts$mean_Possessor_NP_Type_contrast) + .05, y = 80, 
           color = "red", hjust = 0,
           label = paste("median =", round(median(np_type_contrasts$mean_Possessor_NP_Type_contrast), 3))) +
  theme_classic()
```
Try with `party` model.

```{r}
np_type_contrasts_party <- marginal_contrasts(marginal_preds_party, "Possessor_NP_Type")
ggplot(np_type_contrasts_party, aes(x = mean_Possessor_NP_Type_contrast)) +
  geom_vline(xintercept = 0, color = "grey") +
  geom_histogram(color = "black", fill = "gray", bins = 50) +
  geom_vline(xintercept = median(np_type_contrasts_party$mean_Possessor_NP_Type_contrast), color = "red", size = 1) +
  annotate(geom = "text", x = median(np_type_contrasts_party$mean_Possessor_NP_Type_contrast) + .03, y = 70, 
           color = "red", hjust = 0,
           label = paste("median =", round(median(np_type_contrasts_party$mean_Possessor_NP_Type_contrast), 3))) +
  theme_classic() + ggtitle("NP type contrasts in {party} model")
```




```{r}
animacy_contrasts <- marginal_preds |>
  marginal_contrasts("Possessor_Animacy3")
```

```{r}
animacy_contrasts |> 
  summarise(across(6:8, ~ median(.x)))
```



```{r fig.width=6}
library(ggridges)

animacy_contrasts |> 
  pivot_longer(6:8, names_to = "contrast") |> 
  ggplot(aes(value, contrast)) +
  geom_density_ridges() +
  # stat_summary(geom = "pointrange", fun.data = "median_hilow") +
  theme_classic()
```

```{r}
genre_contrasts <- marginal_preds |>
  marginal_contrasts("Genre")
```

```{r}
glimpse(genre_contrasts)
```

We can plot these and color by the comparisons with Press (note the marginal averages above showed a distinct difference between Press and the rest).

```{r fig.width=6}
genre_contrasts |> 
  pivot_longer(6:15, names_to = "contrast") |> 
  mutate(
    color = if_else(grepl("Press", contrast), "press", "nonpress")
  ) |> 
  ggplot(aes(value, contrast, fill = color)) +
  geom_density_ridges() +
  # stat_summary(geom = "pointrange", fun.data = "median_hilow") +
  theme_classic()
```

All the comparisons with Press show long left tails. Because these reflect the difference in predicted probability of GENRE - Press, the left tails tell us that there are many combinations in which the *s*-genitive is predicted to be **less** likely in the stated genre than in Press texts.  


## References

Sönning, Lukas & Jason Grafmiller. 2022. Seeing the wood for the trees: Predictive margins for random forests. Preprint. *PsyArXiv*. [https://doi.org/10.31234/osf.io/jr8yk](https://doi.org/10.31234/osf.io/jr8yk).








